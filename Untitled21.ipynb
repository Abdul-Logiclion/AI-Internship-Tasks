{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhG5AJkDXBIa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4b0a320"
      },
      "source": [
        "# Task\n",
        "Classify music genres using the GTZAN dataset (\"andradaolteanu/gtzan-dataset-music-genre-classification\") by extracting audio features with Librosa and training a multi-class classification model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6fd35c5"
      },
      "source": [
        "## Download the gtzan dataset\n",
        "\n",
        "### Subtask:\n",
        "Download the dataset from Kaggle using the Kagglehub library.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cecf10ea"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the kagglehub library and download the dataset using the provided path.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "310acebe",
        "outputId": "3547b804-d9ab-41ac-c199-351b4df7d53d"
      },
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download the dataset\n",
        "dataset_path = kagglehub.dataset(\"andradaolteanu/gtzan-dataset-music-genre-classification\")\n",
        "\n",
        "# Print the path to the downloaded dataset\n",
        "print(f\"Dataset downloaded to: {dataset_path}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'kagglehub' has no attribute 'dataset'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-730308993.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Download the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdataset_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkagglehub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"andradaolteanu/gtzan-dataset-music-genre-classification\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Print the path to the downloaded dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'kagglehub' has no attribute 'dataset'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5726fa63"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt failed because the `kagglehub` library does not have a `dataset` attribute. I need to find the correct function to download a dataset using `kagglehub`. Based on the `kagglehub` documentation, the correct function is `model_download` for models and `dataset_download` for datasets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8cJw7OsgWK4",
        "outputId": "e7d8917a-dab4-4ec5-e7eb-0f006e4d7ed9"
      },
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download the dataset\n",
        "dataset_path = kagglehub.dataset_download(\"andradaolteanu/gtzan-dataset-music-genre-classification\")\n",
        "\n",
        "# Print the path to the downloaded dataset\n",
        "print(f\"Dataset downloaded to: {dataset_path}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset downloaded to: /kaggle/input/gtzan-dataset-music-genre-classification\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81dba2d2"
      },
      "source": [
        "## Extract audio features\n",
        "\n",
        "### Subtask:\n",
        "Load the audio files and extract features like MFCCs using Librosa.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55fd28c1"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary libraries and define the dataset path.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2771a18"
      },
      "source": [
        "import librosa\n",
        "import os\n",
        "\n",
        "# Define the path to the downloaded dataset\n",
        "dataset_path = os.path.join(dataset_path, 'genres_original')\n",
        "\n",
        "# Initialize empty lists to store features and labels\n",
        "features = []\n",
        "labels = []"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faa69f87"
      },
      "source": [
        "**Reasoning**:\n",
        "Iterate through the dataset, load audio files, extract MFCCs, calculate their mean, and append to the features and labels lists.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "e52f027d",
        "outputId": "45a7c450-af7c-4865-9ddd-2446821ccd94"
      },
      "source": [
        "# Iterate through each genre directory\n",
        "for genre in os.listdir(dataset_path):\n",
        "    genre_path = os.path.join(dataset_path, genre)\n",
        "    # Ensure it's a directory\n",
        "    if os.path.isdir(genre_path):\n",
        "        # Iterate through audio files in each genre directory\n",
        "        for filename in os.listdir(genre_path):\n",
        "            if filename.endswith('.wav'):\n",
        "                audio_path = os.path.join(genre_path, filename)\n",
        "                try:\n",
        "                    # Load the audio file\n",
        "                    y, sr = librosa.load(audio_path, sr=None)\n",
        "\n",
        "                    # Extract MFCCs\n",
        "                    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
        "\n",
        "                    # Calculate the mean of MFCCs\n",
        "                    mfccs_mean = mfccs.mean(axis=1)\n",
        "\n",
        "                    # Append features and labels\n",
        "                    features.append(mfccs_mean)\n",
        "                    labels.append(genre)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading or processing file {audio_path}: {e}\")\n",
        "\n",
        "print(f\"Extracted features for {len(features)} audio files.\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/kaggle/input/gtzan-dataset-music-genre-classification/genres_original/genres_original'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-795849564.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Iterate through each genre directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mgenre\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mgenre_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenre\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Ensure it's a directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenre_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/gtzan-dataset-music-genre-classification/genres_original/genres_original'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "908b62f6"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the dataset path was constructed incorrectly. The `dataset_path` variable already points to the top-level downloaded dataset directory, so joining it with 'genres_original' again results in a redundant path. Correct the dataset path and re-run the feature extraction loop.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "xjlO7tOXgk_P",
        "outputId": "6d995e12-60bf-4ec3-f84f-3b05e75b44e6"
      },
      "source": [
        "# Correct the dataset path\n",
        "dataset_path = '/kaggle/input/gtzan-dataset-music-genre-classification/genres_original'\n",
        "\n",
        "# Iterate through each genre directory\n",
        "for genre in os.listdir(dataset_path):\n",
        "    genre_path = os.path.join(dataset_path, genre)\n",
        "    # Ensure it's a directory\n",
        "    if os.path.isdir(genre_path):\n",
        "        # Iterate through audio files in each genre directory\n",
        "        for filename in os.listdir(genre_path):\n",
        "            if filename.endswith('.wav'):\n",
        "                audio_path = os.path.join(genre_path, filename)\n",
        "                try:\n",
        "                    # Load the audio file\n",
        "                    y, sr = librosa.load(audio_path, sr=None)\n",
        "\n",
        "                    # Extract MFCCs\n",
        "                    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
        "\n",
        "                    # Calculate the mean of MFCCs\n",
        "                    mfccs_mean = mfccs.mean(axis=1)\n",
        "\n",
        "                    # Append features and labels\n",
        "                    features.append(mfccs_mean)\n",
        "                    labels.append(genre)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading or processing file {audio_path}: {e}\")\n",
        "\n",
        "print(f\"Extracted features for {len(features)} audio files.\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/kaggle/input/gtzan-dataset-music-genre-classification/genres_original'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3254704994.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Iterate through each genre directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mgenre\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mgenre_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenre\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Ensure it's a directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/gtzan-dataset-music-genre-classification/genres_original'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9661190"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command still failed because the hardcoded path might not be correct in this environment. The `dataset_path` variable from the first subtask contains the correct path to the downloaded dataset. Reconstruct the dataset path using the `dataset_path` variable and the 'genres_original' subdirectory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "L1zmrDdngnsy",
        "outputId": "de3a47d9-8097-4783-8e2d-319219d42141"
      },
      "source": [
        "# Correct the dataset path using the variable from the first subtask\n",
        "dataset_path_genres = os.path.join(dataset_path, 'genres_original')\n",
        "\n",
        "# Iterate through each genre directory\n",
        "for genre in os.listdir(dataset_path_genres):\n",
        "    genre_path = os.path.join(dataset_path_genres, genre)\n",
        "    # Ensure it's a directory\n",
        "    if os.path.isdir(genre_path):\n",
        "        # Iterate through audio files in each genre directory\n",
        "        for filename in os.listdir(genre_path):\n",
        "            if filename.endswith('.wav'):\n",
        "                audio_path = os.path.join(genre_path, filename)\n",
        "                try:\n",
        "                    # Load the audio file\n",
        "                    y, sr = librosa.load(audio_path, sr=None)\n",
        "\n",
        "                    # Extract MFCCs\n",
        "                    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
        "\n",
        "                    # Calculate the mean of MFCCs\n",
        "                    mfccs_mean = mfccs.mean(axis=1)\n",
        "\n",
        "                    # Append features and labels\n",
        "                    features.append(mfccs_mean)\n",
        "                    labels.append(genre)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading or processing file {audio_path}: {e}\")\n",
        "\n",
        "print(f\"Extracted features for {len(features)} audio files.\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/kaggle/input/gtzan-dataset-music-genre-classification/genres_original/genres_original'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4132553371.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Iterate through each genre directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mgenre\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path_genres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mgenre_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path_genres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenre\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Ensure it's a directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/gtzan-dataset-music-genre-classification/genres_original/genres_original'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c49e4134"
      },
      "source": [
        "**Reasoning**:\n",
        "The error indicates that `dataset_path_genres` is still pointing to an incorrect path. This suggests that the structure within `dataset_path` is not as expected. Let's inspect the contents of `dataset_path` to understand the actual directory structure and then construct the correct path to 'genres_original'.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "xxUuLUbbgvjE",
        "outputId": "3ab8a59f-b806-42a9-afa8-3b77cd983bee"
      },
      "source": [
        "# Inspect the contents of dataset_path to understand the directory structure\n",
        "print(os.listdir(dataset_path))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/kaggle/input/gtzan-dataset-music-genre-classification/genres_original'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2025521453.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Inspect the contents of dataset_path to understand the directory structure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/gtzan-dataset-music-genre-classification/genres_original'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aebf47a7"
      },
      "source": [
        "**Reasoning**:\n",
        "The error indicates that `dataset_path` itself is incorrect and already includes 'genres_original'. Let's go back to the original `dataset_path` from the download step and then correctly construct the path to 'genres_original'.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "BYg5S32ggyxe",
        "outputId": "f3e7b025-fbe3-4aae-f7ad-7d14713a0fad"
      },
      "source": [
        "import os\n",
        "import librosa\n",
        "import kagglehub\n",
        "\n",
        "# Re-download the dataset to get the correct base path in case the variable was overwritten or lost\n",
        "dataset_path_base = kagglehub.dataset_download(\"andradaolteanu/gtzan-dataset-music-genre-classification\")\n",
        "\n",
        "# Define the correct path to the audio files\n",
        "dataset_path_genres = os.path.join(dataset_path_base, 'genres_original')\n",
        "\n",
        "# Initialize empty lists to store features and labels\n",
        "features = []\n",
        "labels = []\n",
        "\n",
        "# Iterate through each genre directory\n",
        "for genre in os.listdir(dataset_path_genres):\n",
        "    genre_path = os.path.join(dataset_path_genres, genre)\n",
        "    # Ensure it's a directory\n",
        "    if os.path.isdir(genre_path):\n",
        "        # Iterate through audio files in each genre directory\n",
        "        for filename in os.listdir(genre_path):\n",
        "            if filename.endswith('.wav'):\n",
        "                audio_path = os.path.join(genre_path, filename)\n",
        "                try:\n",
        "                    # Load the audio file\n",
        "                    y, sr = librosa.load(audio_path, sr=None)\n",
        "\n",
        "                    # Extract MFCCs\n",
        "                    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
        "\n",
        "                    # Calculate the mean of MFCCs\n",
        "                    mfccs_mean = mfccs.mean(axis=1)\n",
        "\n",
        "                    # Append features and labels\n",
        "                    features.append(mfccs_mean)\n",
        "                    labels.append(genre)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading or processing file {audio_path}: {e}\")\n",
        "\n",
        "print(f\"Extracted features for {len(features)} audio files.\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/kaggle/input/gtzan-dataset-music-genre-classification/genres_original'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3008781331.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Iterate through each genre directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mgenre\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path_genres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mgenre_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path_genres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenre\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Ensure it's a directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/gtzan-dataset-music-genre-classification/genres_original'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "969b84c4"
      },
      "source": [
        "## Prepare data for modeling\n",
        "\n",
        "### Subtask:\n",
        "Organize the extracted features and corresponding genre labels into a format suitable for training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70133b8e"
      },
      "source": [
        "**Reasoning**:\n",
        "Convert the features and labels into the required format for training, encode the labels, and split the data into training and testing sets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "1bfd983c",
        "outputId": "4b53752f-aba4-472c-a174-1c13c579feff"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Convert features list to NumPy array\n",
        "features_array = np.array(features)\n",
        "\n",
        "# Convert labels list to Pandas Series\n",
        "labels_series = pd.Series(labels)\n",
        "\n",
        "# Encode genre labels\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(labels_series)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    features_array, encoded_labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Data organized and split successfully.\")\n",
        "print(f\"Shape of training features: {X_train.shape}\")\n",
        "print(f\"Shape of testing features: {X_test.shape}\")\n",
        "print(f\"Shape of training labels: {y_train.shape}\")\n",
        "print(f\"Shape of testing labels: {y_test.shape}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2728003765.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Split data into training and testing sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m X_train, X_test, y_train, y_test = train_test_split(\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mfeatures_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2850\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2851\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2852\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2853\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2481\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2482\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2483\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ea5d022b"
      },
      "source": [
        "## Extract audio features\n",
        "\n",
        "### Subtask:\n",
        "Load the audio files and extract features like MFCCs using Librosa.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd672a39"
      },
      "source": [
        "**Reasoning**:\n",
        "Re-running the code to extract features from audio files to attempt to resolve the file loading error encountered previously.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "053d3f5a",
        "outputId": "5dfae842-1523-4df6-b7a7-e36e3b2c5889"
      },
      "source": [
        "import os\n",
        "import librosa\n",
        "import kagglehub\n",
        "\n",
        "# Re-download the dataset to get the correct base path in case the variable was overwritten or lost\n",
        "dataset_path_base = kagglehub.dataset_download(\"andradaolteanu/gtzan-dataset-music-genre-classification\")\n",
        "\n",
        "# Define the correct path to the audio files\n",
        "dataset_path_genres = os.path.join(dataset_path_base, 'genres_original')\n",
        "\n",
        "# Initialize empty lists to store features and labels\n",
        "features = []\n",
        "labels = []\n",
        "\n",
        "# Iterate through each genre directory\n",
        "for genre in os.listdir(dataset_path_genres):\n",
        "    genre_path = os.path.join(dataset_path_genres, genre)\n",
        "    # Ensure it's a directory\n",
        "    if os.path.isdir(genre_path):\n",
        "        # Iterate through audio files in each genre directory\n",
        "        for filename in os.listdir(genre_path):\n",
        "            if filename.endswith('.wav'):\n",
        "                audio_path = os.path.join(genre_path, filename)\n",
        "                try:\n",
        "                    # Load the audio file\n",
        "                    y, sr = librosa.load(audio_path, sr=None)\n",
        "\n",
        "                    # Extract MFCCs\n",
        "                    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
        "\n",
        "                    # Calculate the mean of MFCCs\n",
        "                    mfccs_mean = mfccs.mean(axis=1)\n",
        "\n",
        "                    # Append features and labels\n",
        "                    features.append(mfccs_mean)\n",
        "                    labels.append(genre)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading or processing file {audio_path}: {e}\")\n",
        "\n",
        "print(f\"Extracted features for {len(features)} audio files.\")\n",
        "\n",
        "# Verify if features and labels lists are populated\n",
        "print(f\"Number of features extracted: {len(features)}\")\n",
        "print(f\"Number of labels extracted: {len(labels)}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/andradaolteanu/gtzan-dataset-music-genre-classification?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.21G/1.21G [00:32<00:00, 40.5MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/root/.cache/kagglehub/datasets/andradaolteanu/gtzan-dataset-music-genre-classification/versions/1/genres_original'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2511015225.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Iterate through each genre directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mgenre\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path_genres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mgenre_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path_genres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenre\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Ensure it's a directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/root/.cache/kagglehub/datasets/andradaolteanu/gtzan-dataset-music-genre-classification/versions/1/genres_original'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc845abe"
      },
      "source": [
        "**Reasoning**:\n",
        "The FileNotFoundError persists, indicating that the `genres_original` directory is still not found at the expected path. I need to inspect the contents of the downloaded dataset directory to understand the actual directory structure and correct the path accordingly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOtHVAN3hPwK",
        "outputId": "8a907910-c273-468b-c9b8-6d0154c5ce69"
      },
      "source": [
        "# List the contents of the downloaded dataset directory to verify the structure\n",
        "print(f\"Contents of the downloaded dataset directory: {os.listdir(dataset_path_base)}\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of the downloaded dataset directory: ['Data']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb5fdec4"
      },
      "source": [
        "**Reasoning**:\n",
        "The contents of the downloaded dataset directory show a single directory named 'Data'. The previous attempts failed because they were looking for 'genres_original' directly under the base path. I need to update the `dataset_path_genres` to include 'Data' in the path.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5ab3250",
        "outputId": "1aca104d-4834-45b2-a965-0936d53a1950"
      },
      "source": [
        "import os\n",
        "import librosa\n",
        "import kagglehub\n",
        "\n",
        "# Re-download the dataset to get the correct base path in case the variable was overwritten or lost\n",
        "dataset_path_base = kagglehub.dataset_download(\"andradaolteanu/gtzan-dataset-music-genre-classification\")\n",
        "\n",
        "# Define the correct path to the audio files, including the 'Data' subdirectory\n",
        "dataset_path_genres = os.path.join(dataset_path_base, 'Data', 'genres_original')\n",
        "\n",
        "# Initialize empty lists to store features and labels\n",
        "features = []\n",
        "labels = []\n",
        "\n",
        "# Iterate through each genre directory\n",
        "for genre in os.listdir(dataset_path_genres):\n",
        "    genre_path = os.path.join(dataset_path_genres, genre)\n",
        "    # Ensure it's a directory\n",
        "    if os.path.isdir(genre_path):\n",
        "        # Iterate through audio files in each genre directory\n",
        "        for filename in os.listdir(genre_path):\n",
        "            if filename.endswith('.wav'):\n",
        "                audio_path = os.path.join(genre_path, filename)\n",
        "                try:\n",
        "                    # Load the audio file\n",
        "                    y, sr = librosa.load(audio_path, sr=None)\n",
        "\n",
        "                    # Extract MFCCs\n",
        "                    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40)\n",
        "\n",
        "                    # Calculate the mean of MFCCs\n",
        "                    mfccs_mean = mfccs.mean(axis=1)\n",
        "\n",
        "                    # Append features and labels\n",
        "                    features.append(mfccs_mean)\n",
        "                    labels.append(genre)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading or processing file {audio_path}: {e}\")\n",
        "\n",
        "print(f\"Extracted features for {len(features)} audio files.\")\n",
        "\n",
        "# Verify if features and labels lists are populated\n",
        "print(f\"Number of features extracted: {len(features)}\")\n",
        "print(f\"Number of labels extracted: {len(labels)}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4033197341.py:26: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  y, sr = librosa.load(audio_path, sr=None)\n",
            "/usr/local/lib/python3.12/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error loading or processing file /kaggle/input/gtzan-dataset-music-genre-classification/Data/genres_original/jazz/jazz.00054.wav: \n",
            "Extracted features for 999 audio files.\n",
            "Number of features extracted: 999\n",
            "Number of labels extracted: 999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93605ba6"
      },
      "source": [
        "## Prepare data for modeling\n",
        "\n",
        "### Subtask:\n",
        "Organize the extracted features and corresponding genre labels into a format suitable for training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5c43e52d"
      },
      "source": [
        "**Reasoning**:\n",
        "Convert the `features` list into a NumPy array, the `labels` list into a Pandas Series, encode the labels, split the data into training and testing sets, and print the shapes of the resulting arrays and series as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9832098a",
        "outputId": "a4db7ec0-d4ca-439c-a808-f52dbb4cd987"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Convert features list to NumPy array\n",
        "features_array = np.array(features)\n",
        "\n",
        "# Convert labels list to Pandas Series\n",
        "labels_series = pd.Series(labels)\n",
        "\n",
        "# Encode genre labels\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(labels_series)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    features_array, encoded_labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Data organized and split successfully.\")\n",
        "print(f\"Shape of training features: {X_train.shape}\")\n",
        "print(f\"Shape of testing features: {X_test.shape}\")\n",
        "print(f\"Shape of training labels: {y_train.shape}\")\n",
        "print(f\"Shape of testing labels: {y_test.shape}\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data organized and split successfully.\n",
            "Shape of training features: (799, 40)\n",
            "Shape of testing features: (200, 40)\n",
            "Shape of training labels: (799,)\n",
            "Shape of testing labels: (200,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28d208c2"
      },
      "source": [
        "## Train a classification model\n",
        "\n",
        "### Subtask:\n",
        "Choose and train a multi-class classification model (e.g., using Scikit-learn or Keras) on the extracted features.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fa286ea"
      },
      "source": [
        "**Reasoning**:\n",
        "Import a multi-class classification model and train it on the training data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87745b5f",
        "outputId": "0037d51b-122e-4ab8-c619-9126aa999297"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Instantiate the model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Model trained successfully.\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model trained successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "173994c1"
      },
      "source": [
        "**Reasoning**:\n",
        "The model training encountered a convergence warning, indicating that increasing max_iter might improve the model. Train the model again with a higher max_iter.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aM2NWubklHPt",
        "outputId": "4f1a141b-80cd-46d0-dc5a-c87a0d0d2938"
      },
      "source": [
        "# Instantiate the model with increased max_iter\n",
        "model = LogisticRegression(max_iter=5000)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Model trained successfully with increased max_iter.\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model trained successfully with increased max_iter.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81cf74fc"
      },
      "source": [
        "## Evaluate the model\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the trained model's performance using appropriate metrics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afd5d290"
      },
      "source": [
        "**Reasoning**:\n",
        "Use the trained model to make predictions on the test set and then calculate and print the accuracy and the classification report.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da70cbd8",
        "outputId": "2729c948-a0f2-4d94-9374-384808d35621"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Generate and print the classification report\n",
        "class_report = classification_report(y_test, y_pred, target_names=label_encoder.classes_)\n",
        "print(\"Classification Report:\")\n",
        "print(class_report)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5400\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       blues       0.60      0.41      0.49        22\n",
            "   classical       0.91      0.71      0.80        28\n",
            "     country       0.65      0.50      0.56        22\n",
            "       disco       0.26      0.29      0.27        21\n",
            "      hiphop       0.48      0.58      0.52        19\n",
            "        jazz       0.39      0.53      0.45        17\n",
            "       metal       0.63      1.00      0.77        12\n",
            "         pop       0.65      0.85      0.74        20\n",
            "      reggae       0.53      0.38      0.44        24\n",
            "        rock       0.27      0.27      0.27        15\n",
            "\n",
            "    accuracy                           0.54       200\n",
            "   macro avg       0.54      0.55      0.53       200\n",
            "weighted avg       0.56      0.54      0.54       200\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97a1495b"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The dataset was successfully downloaded from Kaggle, although initial attempts to locate the data within the downloaded directory failed until the correct path, including the 'Data' subdirectory, was identified.\n",
        "*   MFCC features were successfully extracted from 999 out of 1000 audio files using Librosa; one file ('jazz.00054.wav') caused an error during processing.\n",
        "*   The extracted features (999 samples with 40 features each) and corresponding genre labels were successfully organized, the labels were encoded, and the data was split into training (799 samples) and testing (200 samples) sets.\n",
        "*   A Logistic Regression model was trained on the prepared data. A convergence warning was observed during training even after increasing the maximum iterations.\n",
        "*   The trained model achieved an accuracy of 0.5400 on the test set.\n",
        "*   The classification report showed varying performance across genres, with macro and weighted average F1-scores of 0.53 and 0.54, respectively.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Investigate the file 'jazz.00054.wav' that failed during feature extraction and determine if it can be fixed or should be excluded.\n",
        "*   Explore alternative classification models or feature engineering techniques to improve the model's performance, particularly for genres with lower precision, recall, and F1-scores.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79d2e893"
      },
      "source": [
        "# Task\n",
        "Classify music genres using the GTZAN dataset (\"andradaolteanu/gtzan-dataset-music-genre-classification\") by applying both tabular and image-based approaches (spectrograms with transfer learning) and comparing their results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "902b9855"
      },
      "source": [
        "## Generate spectrograms\n",
        "\n",
        "### Subtask:\n",
        "Convert the audio files into spectrogram images.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6e862e2"
      },
      "source": [
        "**Reasoning**:\n",
        "Iterate through the dataset, load audio files, create and convert spectrograms to decibels, and save them as images in a new directory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "a554c6e9",
        "outputId": "96395e64-14d4-45c9-9a04-e12602ac10bc"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a directory to save the spectrogram images\n",
        "spectrogram_dir = os.path.join(dataset_path_base, 'spectrograms')\n",
        "os.makedirs(spectrogram_dir, exist_ok=True)\n",
        "\n",
        "# Iterate through each genre directory\n",
        "for genre in os.listdir(dataset_path_genres):\n",
        "    genre_path = os.path.join(dataset_path_genres, genre)\n",
        "    # Ensure it's a directory\n",
        "    if os.path.isdir(genre_path):\n",
        "        # Create a subdirectory for each genre within the spectrogram directory\n",
        "        genre_spectrogram_dir = os.path.join(spectrogram_dir, genre)\n",
        "        os.makedirs(genre_spectrogram_dir, exist_ok=True)\n",
        "\n",
        "        # Iterate through audio files in each genre directory\n",
        "        for filename in os.listdir(genre_path):\n",
        "            if filename.endswith('.wav'):\n",
        "                audio_path = os.path.join(genre_path, filename)\n",
        "                try:\n",
        "                    # Load the audio file\n",
        "                    y, sr = librosa.load(audio_path, sr=None)\n",
        "\n",
        "                    # Create a mel spectrogram\n",
        "                    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr)\n",
        "\n",
        "                    # Convert to decibels\n",
        "                    mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
        "\n",
        "                    # Save the spectrogram as an image\n",
        "                    plt.figure(figsize=(10, 4))\n",
        "                    librosa.display.specshow(mel_spectrogram_db, sr=sr, x_axis='time', y_axis='mel')\n",
        "                    plt.colorbar(format='%+2.0f dB')\n",
        "                    plt.title('Mel-frequency spectrogram')\n",
        "                    plt.tight_layout()\n",
        "\n",
        "                    # Define the output path for the spectrogram image\n",
        "                    output_filename = os.path.splitext(filename)[0] + '.png'\n",
        "                    output_path = os.path.join(genre_spectrogram_dir, output_filename)\n",
        "\n",
        "                    # Save the figure and close it\n",
        "                    plt.savefig(output_path)\n",
        "                    plt.close()\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing file {audio_path}: {e}\")\n",
        "\n",
        "print(\"Spectrogram generation complete.\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "[Errno 30] Read-only file system: '/kaggle/input/gtzan-dataset-music-genre-classification/spectrograms'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1586637227.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Create a directory to save the spectrogram images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mspectrogram_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_path_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'spectrograms'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspectrogram_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Iterate through each genre directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/os.py\u001b[0m in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system: '/kaggle/input/gtzan-dataset-music-genre-classification/spectrograms'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "300ecf88"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the target directory for saving spectrograms is a read-only file system. I need to choose a writable directory to save the generated spectrogram images. The `/kaggle/working/` directory is typically writable in this environment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hz1QX6Fl970",
        "outputId": "c6616b51-4b9f-4e21-fabf-449b40220f17"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a directory to save the spectrogram images in a writable location\n",
        "spectrogram_dir = '/kaggle/working/spectrograms'\n",
        "os.makedirs(spectrogram_dir, exist_ok=True)\n",
        "\n",
        "# Iterate through each genre directory\n",
        "for genre in os.listdir(dataset_path_genres):\n",
        "    genre_path = os.path.join(dataset_path_genres, genre)\n",
        "    # Ensure it's a directory\n",
        "    if os.path.isdir(genre_path):\n",
        "        # Create a subdirectory for each genre within the spectrogram directory\n",
        "        genre_spectrogram_dir = os.path.join(spectrogram_dir, genre)\n",
        "        os.makedirs(genre_spectrogram_dir, exist_ok=True)\n",
        "\n",
        "        # Iterate through audio files in each genre directory\n",
        "        for filename in os.listdir(genre_path):\n",
        "            if filename.endswith('.wav'):\n",
        "                audio_path = os.path.join(genre_path, filename)\n",
        "                try:\n",
        "                    # Load the audio file\n",
        "                    y, sr = librosa.load(audio_path, sr=None)\n",
        "\n",
        "                    # Create a mel spectrogram\n",
        "                    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr)\n",
        "\n",
        "                    # Convert to decibels\n",
        "                    mel_spectrogram_db = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
        "\n",
        "                    # Save the spectrogram as an image\n",
        "                    plt.figure(figsize=(10, 4))\n",
        "                    librosa.display.specshow(mel_spectrogram_db, sr=sr, x_axis='time', y_axis='mel')\n",
        "                    plt.colorbar(format='%+2.0f dB')\n",
        "                    plt.title('Mel-frequency spectrogram')\n",
        "                    plt.tight_layout()\n",
        "\n",
        "                    # Define the output path for the spectrogram image\n",
        "                    output_filename = os.path.splitext(filename)[0] + '.png'\n",
        "                    output_path = os.path.join(genre_spectrogram_dir, output_filename)\n",
        "\n",
        "                    # Save the figure and close it\n",
        "                    plt.savefig(output_path)\n",
        "                    plt.close()\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing file {audio_path}: {e}\")\n",
        "\n",
        "print(\"Spectrogram generation complete.\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1700439665.py:22: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  y, sr = librosa.load(audio_path, sr=None)\n",
            "/usr/local/lib/python3.12/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing file /kaggle/input/gtzan-dataset-music-genre-classification/Data/genres_original/jazz/jazz.00054.wav: \n",
            "Spectrogram generation complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a947ca7"
      },
      "source": [
        "## Prepare image data for modeling\n",
        "\n",
        "### Subtask:\n",
        "Organize the spectrogram images and corresponding genre labels into a format suitable for training a CNN model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6627483d"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary library, define parameters, and create data generators for training and validation using ImageDataGenerator to prepare the spectrogram images for CNN model training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e9d159e",
        "outputId": "b44658f9-23ca-4e20-ca72-7158751a2874"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Define the path to the directory containing the generated spectrograms\n",
        "spectrogram_dir = '/kaggle/working/spectrograms'\n",
        "\n",
        "# Define the image dimensions\n",
        "img_height = 216  # Adjust based on the generated spectrogram image size\n",
        "img_width = 360   # Adjust based on the generated spectrogram image size\n",
        "\n",
        "# Define the batch size\n",
        "batch_size = 32\n",
        "\n",
        "# Create an instance of ImageDataGenerator with a validation split\n",
        "image_generator = ImageDataGenerator(validation_split=0.2)\n",
        "\n",
        "# Create training data generator\n",
        "train_generator = image_generator.flow_from_directory(\n",
        "    spectrogram_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "# Create validation data generator\n",
        "validation_generator = image_generator.flow_from_directory(\n",
        "    spectrogram_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Print the number of images found for training and validation\n",
        "print(f\"Found {train_generator.samples} images belonging to {train_generator.num_classes} classes for training.\")\n",
        "print(f\"Found {validation_generator.samples} images belonging to {validation_generator.num_classes} classes for validation.\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 800 images belonging to 10 classes.\n",
            "Found 199 images belonging to 10 classes.\n",
            "Found 800 images belonging to 10 classes for training.\n",
            "Found 199 images belonging to 10 classes for validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e51bd55"
      },
      "source": [
        "## Apply transfer learning\n",
        "\n",
        "### Subtask:\n",
        "Load a pre-trained Convolutional Neural Network (CNN) model and fine-tune it on the spectrogram dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d2568a4"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary libraries, load a pre-trained CNN model without its top, freeze its layers, add a new classification head, and compile the complete model as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 939
        },
        "id": "b0909944",
        "outputId": "0346a645-e0bd-4cef-f5e7-b96f694ffdeb"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Flatten, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import VGG16 # Example pre-trained model\n",
        "\n",
        "# Define input shape matching spectrograms (including 3 channels)\n",
        "input_shape = (img_height, img_width, 3)\n",
        "img_input = Input(shape=input_shape)\n",
        "\n",
        "# Load pre-trained VGG16 model without the top classification layer\n",
        "# Use the functional API to connect the input\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_tensor=img_input)\n",
        "\n",
        "# Freeze the layers of the base model\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Build a new classification head\n",
        "x = Flatten()(base_model.output)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "# The number of units in the final Dense layer should be the number of classes\n",
        "# which can be obtained from the train_generator\n",
        "predictions = Dense(train_generator.num_classes, activation='softmax')(x)\n",
        "\n",
        "# Create the complete model\n",
        "model_cnn = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Compile the model\n",
        "model_cnn.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "model_cnn.summary()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m216\u001b[0m, \u001b[38;5;34m360\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block1_conv1 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m216\u001b[0m, \u001b[38;5;34m360\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │         \u001b[38;5;34m1,792\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block1_conv2 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m216\u001b[0m, \u001b[38;5;34m360\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │        \u001b[38;5;34m36,928\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block1_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m108\u001b[0m, \u001b[38;5;34m180\u001b[0m, \u001b[38;5;34m64\u001b[0m)   │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block2_conv1 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m108\u001b[0m, \u001b[38;5;34m180\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block2_conv2 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m108\u001b[0m, \u001b[38;5;34m180\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │       \u001b[38;5;34m147,584\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block2_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block3_conv1 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m295,168\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block3_conv2 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m590,080\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block3_conv3 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m90\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │       \u001b[38;5;34m590,080\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block3_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m27\u001b[0m, \u001b[38;5;34m45\u001b[0m, \u001b[38;5;34m256\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block4_conv1 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m27\u001b[0m, \u001b[38;5;34m45\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │     \u001b[38;5;34m1,180,160\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block4_conv2 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m27\u001b[0m, \u001b[38;5;34m45\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │     \u001b[38;5;34m2,359,808\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block4_conv3 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m27\u001b[0m, \u001b[38;5;34m45\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │     \u001b[38;5;34m2,359,808\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block4_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block5_conv1 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │     \u001b[38;5;34m2,359,808\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block5_conv2 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │     \u001b[38;5;34m2,359,808\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block5_conv3 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m512\u001b[0m)    │     \u001b[38;5;34m2,359,808\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block5_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m512\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m33792\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │     \u001b[38;5;34m8,651,008\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m2,570\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">216</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">360</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block1_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">216</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">360</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block1_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">216</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">360</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block1_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">108</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">180</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block2_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">108</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">180</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block2_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">108</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">180</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block2_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block3_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block3_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block3_conv3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">90</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block3_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">45</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block4_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">45</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,180,160</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block4_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">45</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block4_conv3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">45</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block4_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block5_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block5_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block5_conv3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)    │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ block5_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">33792</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">8,651,008</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,570</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m23,368,266\u001b[0m (89.14 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,368,266</span> (89.14 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m8,653,578\u001b[0m (33.01 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">8,653,578</span> (33.01 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m14,714,688\u001b[0m (56.13 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> (56.13 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c83ded4"
      },
      "source": [
        "## Train the image-based model\n",
        "\n",
        "### Subtask:\n",
        "Train the fine-tuned CNN model on the prepared image data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60b4f52f"
      },
      "source": [
        "**Reasoning**:\n",
        "Train the compiled CNN model using the training and validation data generators for a specified number of epochs and store the training history.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d0ae613",
        "outputId": "f3843c4e-6b93-495e-ce4d-6d7085fe2f1e"
      },
      "source": [
        "# Train the model using the data generators\n",
        "epochs = 15  # You can adjust this number\n",
        "history = model_cnn.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator\n",
        ")\n",
        "\n",
        "print(\"CNN model training complete.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc212422"
      },
      "source": [
        "## Evaluate the image-based model\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the trained image-based model's performance using appropriate metrics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfc0c423"
      },
      "source": [
        "**Reasoning**:\n",
        "Use the trained CNN model to predict on the validation set, get the true labels, convert predictions to class labels, and then calculate and print the accuracy and classification report to evaluate the model's performance as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "bf97eb3d",
        "outputId": "7100a15b-ec30-450f-8986-9b818025b5b1"
      },
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Get predictions from the model on the validation data\n",
        "y_pred_probs = model_cnn.predict(validation_generator)\n",
        "\n",
        "# Get the true labels from the validation generator\n",
        "y_true = validation_generator.classes\n",
        "\n",
        "# Convert predicted probabilities to class labels\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy_cnn = accuracy_score(y_true, y_pred)\n",
        "print(f\"Accuracy of the CNN model on the validation set: {accuracy_cnn:.4f}\")\n",
        "\n",
        "# Get the class names from the generator\n",
        "class_names = list(validation_generator.class_indices.keys())\n",
        "\n",
        "# Generate and print the classification report\n",
        "class_report_cnn = classification_report(y_true, y_pred, target_names=class_names)\n",
        "print(\"Classification Report for CNN model:\")\n",
        "print(class_report_cnn)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model_cnn' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2592704665.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Get predictions from the model on the validation data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0my_pred_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_cnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Get the true labels from the validation generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model_cnn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7959c6c"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because the `model_cnn` variable was not defined in the current session. I need to re-create and train the CNN model before evaluating it. I will re-run the code to build, compile, and train the CNN model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "LBWr598msC6Q",
        "outputId": "04d5af4c-66b5-4221-9182-4583e116ebc9"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Flatten, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import VGG16 # Example pre-trained model\n",
        "\n",
        "# Define input shape matching spectrograms (including 3 channels)\n",
        "# img_height and img_width are defined in a previous cell\n",
        "input_shape = (img_height, img_width, 3)\n",
        "img_input = Input(shape=input_shape)\n",
        "\n",
        "# Load pre-trained VGG16 model without the top classification layer\n",
        "# Use the functional API to connect the input\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_tensor=img_input)\n",
        "\n",
        "# Freeze the layers of the base model\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Build a new classification head\n",
        "x = Flatten()(base_model.output)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "# The number of units in the final Dense layer should be the number of classes\n",
        "# which can be obtained from the train_generator\n",
        "predictions = Dense(train_generator.num_classes, activation='softmax')(x)\n",
        "\n",
        "# Create the complete model\n",
        "model_cnn = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Compile the model\n",
        "model_cnn.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# Train the model using the data generators\n",
        "epochs = 15  # You can adjust this number\n",
        "history = model_cnn.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator\n",
        ")\n",
        "\n",
        "print(\"CNN model training complete.\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'img_height' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-706808996.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Define input shape matching spectrograms (including 3 channels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# img_height and img_width are defined in a previous cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimg_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mimg_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'img_height' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53f28a32"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because `img_height` and `img_width` were not defined in the current code block. I need to include the definition of these variables from the cell where the data generators were set up, and then proceed with building, compiling, and training the CNN model, followed by evaluating it as per the subtask instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "xpC67wTDsJED",
        "outputId": "fd8d44c5-7c80-4d38-fab6-c7397e0c3db6"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Flatten, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import VGG16 # Example pre-trained model\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Define the image dimensions (copied from the data preparation step)\n",
        "img_height = 216\n",
        "img_width = 360\n",
        "\n",
        "# Define input shape matching spectrograms (including 3 channels)\n",
        "input_shape = (img_height, img_width, 3)\n",
        "img_input = Input(shape=input_shape)\n",
        "\n",
        "# Load pre-trained VGG16 model without the top classification layer\n",
        "# Use the functional API to connect the input\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_tensor=img_input)\n",
        "\n",
        "# Freeze the layers of the base model\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Build a new classification head\n",
        "x = Flatten()(base_model.output)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "# The number of units in the final Dense layer should be the number of classes\n",
        "# which can be obtained from the train_generator\n",
        "predictions = Dense(train_generator.num_classes, activation='softmax')(x)\n",
        "\n",
        "# Create the complete model\n",
        "model_cnn = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Compile the model\n",
        "model_cnn.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# Train the model using the data generators\n",
        "epochs = 15  # You can adjust this number\n",
        "history = model_cnn.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator\n",
        ")\n",
        "\n",
        "print(\"CNN model training complete.\")\n",
        "\n",
        "# Get predictions from the model on the validation data\n",
        "y_pred_probs = model_cnn.predict(validation_generator)\n",
        "\n",
        "# Get the true labels from the validation generator\n",
        "y_true = validation_generator.classes\n",
        "\n",
        "# Convert predicted probabilities to class labels\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy_cnn = accuracy_score(y_true, y_pred)\n",
        "print(f\"Accuracy of the CNN model on the validation set: {accuracy_cnn:.4f}\")\n",
        "\n",
        "# Get the class names from the generator\n",
        "class_names = list(validation_generator.class_indices.keys())\n",
        "\n",
        "# Generate and print the classification report\n",
        "class_report_cnn = classification_report(y_true, y_pred, target_names=class_names)\n",
        "print(\"Classification Report for CNN model:\")\n",
        "print(class_report_cnn)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_generator' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1787527020.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# The number of units in the final Dense layer should be the number of classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# which can be obtained from the train_generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Create the complete model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_generator' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "407375ce"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed because `train_generator` and `validation_generator` were not defined in the current code block. I need to include the code to create these data generators, then define image dimensions, build, compile, and train the CNN model, and finally evaluate it as per the subtask instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTZNF9qPsNDC",
        "outputId": "7d085449-1cf8-448d-ed34-df2a8839d100"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Flatten, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import VGG16 # Example pre-trained model\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator # Import ImageDataGenerator\n",
        "\n",
        "# Define the path to the directory containing the generated spectrograms (assuming it's still valid)\n",
        "spectrogram_dir = '/kaggle/working/spectrograms'\n",
        "\n",
        "# Define the image dimensions (copied from the data preparation step)\n",
        "img_height = 216\n",
        "img_width = 360\n",
        "\n",
        "# Define the batch size (copied from the data preparation step)\n",
        "batch_size = 32\n",
        "\n",
        "# Create an instance of ImageDataGenerator with a validation split (copied from data preparation step)\n",
        "image_generator = ImageDataGenerator(validation_split=0.2)\n",
        "\n",
        "# Create training data generator (copied from data preparation step)\n",
        "train_generator = image_generator.flow_from_directory(\n",
        "    spectrogram_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "# Create validation data generator (copied from data preparation step)\n",
        "validation_generator = image_generator.flow_from_directory(\n",
        "    spectrogram_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "\n",
        "# Define input shape matching spectrograms (including 3 channels)\n",
        "input_shape = (img_height, img_width, 3)\n",
        "img_input = Input(shape=input_shape)\n",
        "\n",
        "# Load pre-trained VGG16 model without the top classification layer\n",
        "# Use the functional API to connect the input\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_tensor=img_input)\n",
        "\n",
        "# Freeze the layers of the base model\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Build a new classification head\n",
        "x = Flatten()(base_model.output)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "# The number of units in the final Dense layer should be the number of classes\n",
        "# which can be obtained from the train_generator\n",
        "predictions = Dense(train_generator.num_classes, activation='softmax')(x)\n",
        "\n",
        "# Create the complete model\n",
        "model_cnn = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Compile the model\n",
        "model_cnn.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# Train the model using the data generators\n",
        "epochs = 15  # You can adjust this number\n",
        "history = model_cnn.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    validation_data=validation_generator\n",
        ")\n",
        "\n",
        "print(\"CNN model training complete.\")\n",
        "\n",
        "# Get predictions from the model on the validation data\n",
        "y_pred_probs = model_cnn.predict(validation_generator)\n",
        "\n",
        "# Get the true labels from the validation generator\n",
        "y_true = validation_generator.classes\n",
        "\n",
        "# Convert predicted probabilities to class labels\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy_cnn = accuracy_score(y_true, y_pred)\n",
        "print(f\"Accuracy of the CNN model on the validation set: {accuracy_cnn:.4f}\")\n",
        "\n",
        "# Get the class names from the generator\n",
        "class_names = list(validation_generator.class_indices.keys())\n",
        "\n",
        "# Generate and print the classification report\n",
        "class_report_cnn = classification_report(y_true, y_pred, target_names=class_names)\n",
        "print(\"Classification Report for CNN model:\")\n",
        "print(class_report_cnn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 800 images belonging to 10 classes.\n",
            "Found 199 images belonging to 10 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m954s\u001b[0m 38s/step - accuracy: 0.1705 - loss: 155.8791 - val_accuracy: 0.3216 - val_loss: 25.8018\n",
            "Epoch 2/15\n",
            "\u001b[1m12/25\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m6:24\u001b[0m 30s/step - accuracy: 0.5276 - loss: 15.5618"
          ]
        }
      ]
    }
  ]
}